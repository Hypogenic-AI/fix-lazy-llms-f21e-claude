{"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "year": 2023, "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, T. Griffiths, Yuan Cao, Karthik Narasimhan", "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "relevance": 3, "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.", "citations": 3301}
{"title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "year": 2023, "authors": "Maciej Besta, Nils Blach, Ale\u0161 Kub\u00ed\u010dek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, H. Niewiadomski, P. Nyczyk, Torsten Hoefler", "url": "https://www.semanticscholar.org/paper/aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3", "relevance": 3, "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks", "citations": 1108}
{"title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models", "year": 2023, "authors": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim", "url": "https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e", "relevance": 3, "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.", "citations": 566}
{"title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022, "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, Denny Zhou", "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5", "relevance": 3, "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "citations": 15323}
{"title": "Large Language Models are Zero-Shot Reasoners", "year": 2022, "authors": "Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa", "url": "https://api.semanticscholar.org/CorpusId:249017743", "relevance": 3, "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.", "citations": 6322}
{"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "year": 2022, "authors": "Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, D. Schuurmans, O. Bousquet, Quoc Le, Ed H. Chi", "url": "https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321", "relevance": 3, "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.", "citations": 1529}
{"title": "Large Language Models Are Human-Level Prompt Engineers", "year": 2022, "authors": "Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba", "url": "https://api.semanticscholar.org/CorpusId:253265328", "relevance": 3, "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.", "citations": 1206}
{"title": "Complexity-Based Prompting for Multi-Step Reasoning", "year": 2022, "authors": "Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, Tushar Khot", "url": "https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a", "relevance": 3, "abstract": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.", "citations": 556}
{"title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks", "year": 2022, "authors": "Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen", "url": "https://www.semanticscholar.org/paper/6c943670dca38bfc7c8b477ae7c2d1fba1ad3691", "relevance": 3, "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts", "citations": 1131}
{"title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks", "year": 2022, "authors": "Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal", "url": "https://www.semanticscholar.org/paper/07955e96cbd778d0ae2a68f09d073b866dd84c2a", "relevance": 3, "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.", "citations": 611}
{"title": "Towards Reasoning in Large Language Models: A Survey", "year": 2022, "authors": "Jie Huang, K. Chang", "url": "https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45", "relevance": 3, "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.", "citations": 825}
{"title": "Multi-Step Reasoning with Large Language Models, a Survey", "year": 2024, "authors": "A. Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas B\u00e4ck", "url": "https://www.semanticscholar.org/paper/d4f5c6b4ffc60ae04289a40fb0dcce136a122511", "relevance": 3, "abstract": "Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.", "citations": 109}
{"title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models", "year": 2023, "authors": "Bilgehan Sel, Ahmad S. Al-Tawaha, Vanshaj Khattar, Lucy Wang, R. Jia, Ming Jin", "url": "https://www.semanticscholar.org/paper/fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b", "relevance": 3, "abstract": "Current literature, aiming to surpass the\"Chain-of-Thought\"approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io.", "citations": 99}
{"title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought", "year": 2022, "authors": "Boshi Wang, Xiang Deng, Huan Sun", "url": "https://www.semanticscholar.org/paper/3f4d11971f2c64be9125a7fe99c019588bbebf16", "relevance": 3, "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a \u201cchain of thought\u201d for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.", "citations": 120}
{"title": "Reasoning with Language Model Prompting: A Survey", "year": 2022, "authors": "Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen", "url": "https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc", "relevance": 3, "abstract": "Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).", "citations": 403}
{"title": "Chain of Thoughtlessness? An Analysis of CoT in Planning", "year": 2024, "authors": "Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati", "url": "https://www.semanticscholar.org/paper/6c9c0338f1526437b7cd3b9ccec1fff7feafb14c", "relevance": 3, "abstract": "Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.", "citations": 107}
{"title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost", "year": 2024, "authors": "Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, F. Giacomelli", "url": "https://www.semanticscholar.org/paper/a77d28e13ff34f34b8d1114e603bff074ee2b056", "relevance": 3, "abstract": "Today's large language models (LLMs) can solve challenging question-answering tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have gained attention for enhancing the explanation and correctness of outputs. However, many models and techniques tend to produce excessively verbose and lengthy answers, leading to issues with both conciseness and generation time. To address this, this paper analyzes the impact of output lengths on LLM inference pipelines by introducing and proposing novel metrics to evaluate the \\textit{correct conciseness} of a model and related prompting techniques. Then, we examine the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to produce more concise outputs. To better understand the effects of such a prompt, we also introduce two additional scores for analyzing the conciseness, measured in terms of redundancy and information flow in generated answers. Experiments on pretrained LLMs and multiple datasets demonstrate the benefits of the proposed metrics and the effectiveness of CCoT across different models.", "citations": 83}
{"title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks", "year": 2024, "authors": "Shubham Vatsal, Harsh Dubey", "url": "https://www.semanticscholar.org/paper/621b429b0a1537473b40ead23a0e0613e07debbd", "relevance": 3, "abstract": "Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.", "citations": 75}
{"title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods", "year": 2025, "authors": "Lilian Some, Wenli Yang, Michael Bain, B. Kang", "url": "https://www.semanticscholar.org/paper/62a614192682fa5e7790b2dba4effcddda0d9113", "relevance": 3, "abstract": "", "citations": 28}
{"title": "Autonomous Prompt Engineering in Large Language Models", "year": 2024, "authors": "Daan Kepel, Konstantina Valogianni", "url": "https://www.semanticscholar.org/paper/8dbd0e063f9ee22b43c77dc3162e49b7853a2423", "relevance": 3, "abstract": "Prompt engineering is a crucial yet challenging task for optimizing the performance of large language models (LLMs) on customized tasks. This pioneering research introduces the Automatic Prompt Engineering Toolbox (APET), which enables GPT-4 to autonomously apply prompt engineering techniques. By leveraging sophisticated strategies such as Expert Prompting, Chain of Thought, and Tree of Thoughts, APET empowers GPT-4 to dynamically optimize prompts, resulting in substantial improvements in tasks like Word Sorting (4.4% increase) and Geometric Shapes (6.8% increase). Despite encountering challenges in complex tasks such as Checkmate in One (-14.8%), these findings demonstrate the transformative potential of APET in automating complex prompt optimization processes without the use of external data. Overall, this research represents a significant leap in AI development, presenting a robust framework for future innovations in autonomous AI systems and highlighting the ability of GPT-4 to bring prompt engineering theory to practice. It establishes a foundation for enhancing performance in complex task performance and broadening the practical applications of these techniques in real-world scenarios.", "citations": 13}
{"title": "Unlocking Structured Thinking in Language Models with Cognitive Prompting", "year": 2024, "authors": "Oliver Kramer, Jill Baumann", "url": "https://www.semanticscholar.org/paper/770811f516d8842f3460c5a651511ed0d594ee5b", "relevance": 3, "abstract": "We propose cognitive prompting as a novel approach to guide problem-solving in large language models (LLMs) through structured, human-like cognitive operations, such as goal clarification, decomposition, filtering, abstraction, and pattern recognition. By employing systematic, step-by-step reasoning, cognitive prompting enables LLMs to tackle complex, multi-step tasks more efficiently. We introduce three variants: a deterministic sequence of cognitive operations, a self-adaptive variant in which the LLM dynamically selects the sequence of cognitive operations, and a hybrid variant that uses generated correct solutions as few-shot chain-of-thought prompts. Experiments with LLaMA, Gemma~2, and Qwen models in each two sizes on the arithmetic reasoning benchmark GSM8K demonstrate that cognitive prompting significantly improves performance compared to standard question answering.", "citations": 9}
{"title": "Active Prompting with Chain-of-Thought for Large Language Models", "year": 2023, "authors": "Shizhe Diao, Pengcheng Wang, Yong Lin, Xiang Liu, Tong Zhang", "url": "https://www.semanticscholar.org/paper/3fc3460c4554a28e489a0ea6ef067b79b7d301d9", "relevance": 3, "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.", "citations": 192}
{"title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search", "year": 2024, "authors": "Murong Yue, Wenlin Yao, Haitao Mi, Dian Yu, Ziyu Yao, Dong Yu", "url": "https://www.semanticscholar.org/paper/49cb98ea683f752cf9b10a136b382a9d1e684edc", "relevance": 3, "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called\"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.", "citations": 15}
{"title": "Working Document - Formalising Software Requirements with Large Language Models", "year": 2025, "authors": "Arshad Beg, Diarmuid O'Donoghue, Rosemary Monahan", "url": "https://www.semanticscholar.org/paper/a62939abfe15a4f6cf4472de88a4327eaf7dac68", "relevance": 3, "abstract": "This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is: [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025; [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.", "citations": 0}
{"title": "Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers", "year": 2025, "authors": "Alice Rueda, Mohammed S. Hassan, Argyrios Perivolaris, Bazen Gashaw Teferra, Reza Samavi, Sirisha Rambhatla, Yuqi Wu, Yanbo Zhang, Bo Cao, Divya Sharma, Sridhar Krishnan Venkat Bhat", "url": "https://www.semanticscholar.org/paper/922c5c2eb78da92c5a5ca3b9813694756c8e53d5", "relevance": 3, "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and problem-solving across various domains. However, their ability to perform complex, multi-step reasoning task-essential for applications in science, medicine, and law-remains an area of active investigation. This paper examines the reasoning capabilities of contemporary LLMs, analyzing their strengths, limitations, and potential for improvement. The study uses prompt engineering techniques on the Graduate-Level GoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o. Five popular prompt engineering techniques and two tailored promptings were tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our findings indicate that while LLMs exhibit emergent reasoning abilities, they often rely on pattern recognition rather than true logical inference, leading to inconsistencies in complex problem-solving. The results indicated that self-consistency outperformed the other prompt engineering technique with an accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%) outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and CoT (43.75%). Self-consistency performed the second worst in explaining the answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have the best scientific reasoning. We propose a research agenda aimed at bridging these gaps by integrating structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies. By critically evaluating the reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse on the future of artificial general intelligence and the development of more robust, trustworthy AI systems.", "citations": 11}
{"title": "A Short Survey on Formalising Software Requirements using Large Language Models", "year": 2025, "authors": "Arshad Beg, Diarmuid O'Donoghue, Rosemary Monahan", "url": "https://www.semanticscholar.org/paper/1110db17cb53984a94d0d6129450f544a0c0e630", "relevance": 3, "abstract": "This paper presents a focused literature survey on the use of large language models (LLM) to assist in writing formal specifications for software. A summary of thirty-five key papers is presented, including examples for specifying programs written in Dafny, C and Java. This paper arose from the project VERIFAI - Traceability and verification of natural language requirements that addresses the challenges in writing formal specifications from requirements that are expressed in natural language. Our methodology employed multiple academic databases to identify relevant research. The AI-assisted tool Elicit facilitated the initial paper selection, which were manually screened for final selection. The survey provides valuable insights and future directions for utilising LLMs while formalising software requirements.", "citations": 3}
{"title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond", "year": 2024, "authors": "Harsha Nori, N. Usuyama, Nicholas King, S. McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz", "url": "https://www.semanticscholar.org/paper/4401c9786e97cf3c3d76d08efd11756923b9ce96", "relevance": 3, "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.", "citations": 30}
{"title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "year": 2025, "authors": "Yanjian Zhang, Guillaume Wisniewski, Nadi Tomeh, Thierry Charnois", "url": "https://www.semanticscholar.org/paper/c4d8ecdcd7bf884cb51d0cee0d571d88ee0f9530", "relevance": 3, "abstract": "Human reasoning involves different strategies, each suited to specific problems. Prior work shows that large language model (LLMs) tend to favor a single reasoning strategy, potentially limiting their effectiveness in diverse reasoning challenges. In this work, we investigate whether prompting can control LLMs reasoning strategies and assess its impact on logical problem-solving. While our experiments show that no single strategy consistently improves accuracy, performance could be enhanced if models could adaptively choose the optimal strategy. We propose methods to guide LLMs in strategy selection, highlighting new ways to refine their reasoning abilities.", "citations": 0}
{"title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting", "year": 2024, "authors": "Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen", "url": "https://www.semanticscholar.org/paper/2f97fe9ce214394151536118b6e1f0bdd3b190ee", "relevance": 3, "abstract": "Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify\u2014alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.", "citations": 21}
{"title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment", "year": 2025, "authors": "Paul Tschisgale, Holger Maus, Fabian Kieser, Ben Kroehs, Stefan Petersen, Peter Wulff", "url": "https://www.semanticscholar.org/paper/cbd4012c8934c97fda6afd96ab56f0d115ec1840", "relevance": 3, "abstract": "Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.", "citations": 10}
{"title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning", "year": 2024, "authors": "Santosh Kumar Radha, Yasamin Nouri Jelyani, A. Ghukasyan, O. Goktas", "url": "https://www.semanticscholar.org/paper/287aa18cec7808fb5364d6ec25099f59c5014db7", "relevance": 3, "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating\"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.", "citations": 11}
{"title": "Demystifying Chains, Trees, and Graphs of Thoughts", "year": 2024, "authors": "Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, J\u00fcrgen M\u00fcller, Lukas Gianinazzi, Ale\u0161 Kub\u00ed\u010dek, H. Niewiadomski, Aidan O\u2019mahony, O. Mutlu, Torsten Hoefler", "url": "https://www.semanticscholar.org/paper/eff9d7ed06f30f121d30ee13802a11f172ef66f4", "relevance": 3, "abstract": "The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models\u2019 (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM\u2019s capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.", "citations": 56}
{"title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions", "year": 2024, "authors": "Laur\u00e8ne Vaugrante, Mathias Niepert, Thilo Hagendorff", "url": "https://www.semanticscholar.org/paper/fbd6e63300be0401ef646eb6e163835c85c276c7", "relevance": 3, "abstract": "In an era where large language models (LLMs) are increasingly integrated into a wide range of everyday applications, research into these models' behavior has surged. However, due to the novelty of the field, clear methodological guidelines are lacking. This raises concerns about the replicability and generalizability of insights gained from research on LLM behavior. In this study, we discuss the potential risk of a replication crisis and support our concerns with a series of replication experiments focused on prompt engineering techniques purported to influence reasoning abilities in LLMs. We tested GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on the chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well as Re-Reading prompt engineering techniques, using manually double-checked subsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE, ScienceQA, and StrategyQA. Our findings reveal a general lack of statistically significant differences across nearly all techniques tested, highlighting, among others, several methodological weaknesses in previous research. We propose a forward-looking approach that includes developing robust methodologies for evaluating LLMs, establishing sound benchmarks, and designing rigorous experimental frameworks to ensure accurate and reliable assessments of model outputs.", "citations": 8}
{"title": "Irony Detection, Reasoning, and Understanding in Zero-Shot Learning", "year": 2025, "authors": "Peiling Yi, Yuhan Xia, Yunfei Long", "url": "https://www.semanticscholar.org/paper/7079ed3d06b3a4c65558d4173362232cef6ff8dd", "relevance": 3, "abstract": "Irony is a powerful figurative language (FL) on social media that can potentially mislead various NLP tasks, such as recommendation systems, misinformation checks, and sentiment analysis. Understanding the implicit meaning of this kind of subtle language is an essential step to mitigate the negative impact of irony in NLP tasks. However, existing efforts are limited to domain-specific datasets and struggle to generalize across diverse real-world scenarios. Moreover, reasoning for model decisions that accurately capture semantic and affective meaning remains underexplored. To address these limitations, this article proposes a conceptual framework called irony-focused detection, analysis, and decision prompting (IDADP), which leverages large language models\u2019 (LLMs) in-context learning capabilities to detect irony and generate humanlike explanations across diverse datasets and platforms without prior training on ironic samples. Extensive experiments on six widely used irony detection datasets, utilizing two LLMs (GPT and Gemini), demonstrate that IDADP consistently outperforms six competitive zero-shot baselines and approaches the performance of three fine-tuned supervised learning baselines. Additionally, we examine GPT\u2019s ability to understand the true intent behind ironic text within the IDADP framework, highlighting its strong potential to recognize and interpret statements where the intended meaning differs from or contrasts with the literal meaning. Furthermore, we conduct qualitative analyses to identify remaining challenges. This work, in turn, opens an avenue for transparent decision-making in irony detection.", "citations": 6}
{"title": "Conceptual Metaphor Theory as a Prompting Paradigm for Large Language Models", "year": 2025, "authors": "Oliver Kramer", "url": "https://www.semanticscholar.org/paper/70b8f12baeebce4a964433c808c3bd4253b20283", "relevance": 3, "abstract": "We introduce Conceptual Metaphor Theory (CMT) as a framework for enhancing large language models (LLMs) through cognitive prompting in complex reasoning tasks. CMT leverages metaphorical mappings to structure abstract reasoning, improving models' ability to process and explain intricate concepts. By incorporating CMT-based prompts, we guide LLMs toward more structured and human-like reasoning patterns. To evaluate this approach, we compare four native models (Llama3.2, Phi3, Gemma2, and Mistral) against their CMT-augmented counterparts on benchmark tasks spanning domain-specific reasoning, creative insight, and metaphor interpretation. Responses were automatically evaluated using the Llama3.3 70B model. Experimental results indicate that CMT prompting significantly enhances reasoning accuracy, clarity, and metaphorical coherence, outperforming baseline models across all evaluated tasks.", "citations": 1}
{"title": "From Machine Learning-Based to LLM-Enhanced: An Application-Focused Analysis of How Social IoT Benefits from LLMs", "year": 2025, "authors": "Lijie Yang, Runbo Su", "url": "https://www.semanticscholar.org/paper/8f31d13d74092aa019ddbef1cc24f03e3351f38f", "relevance": 3, "abstract": "Recent advancements in large language models (LLMs) have added a transformative dimension to the social Internet of Things (SIoT), which is the combination of social networks and IoT. With LLMs\u2019 natural language understanding and data synthesis capabilities, LLMs are regarded as strong tools to enhance SIoT applications such as recommendation, search, and data management. This application-focused review synthesizes the latest related research by identifying both the synergies and the current research gaps at the intersection of LLMs and SIoT, as well as the evolutionary road from machine learning-based solutions to LLM-enhanced ones.", "citations": 2}
{"title": "A Survey on the Integration of Generative AI for Critical Thinking in Mobile Networks", "year": 2024, "authors": "Athanasios Karapantelakis, Alexandros Nikou, Ajay Kattepur, Jean Martins, Leonid Mokrushin, S. Mohalik, Marin Orlic, Aneta Vulgarakis Feljan", "url": "https://www.semanticscholar.org/paper/ee5e53987c35fdfbd09e7564634c673871e5cab6", "relevance": 3, "abstract": "In the near future, mobile networks are expected to broaden their services and coverage to accommodate a larger user base and diverse user needs. Thus, they will increasingly rely on artificial intelligence (AI) to manage network operation and control costs, undertaking complex decision-making roles. This shift will necessitate the application of techniques that incorporate critical thinking abilities, including reasoning and planning. Symbolic AI techniques already facilitate critical thinking based on existing knowledge. Yet, their use in telecommunications is hindered by the high cost of mostly manual curation of this knowledge and high computational complexity of reasoning tasks. At the same time, there is a spurt of innovations in industries such as telecommunications due to Generative AI (GenAI) technologies, operating independently of human-curated knowledge. However, their capacity for critical thinking remains uncertain. This paper aims to address this gap by examining the current status of GenAI algorithms with critical thinking capabilities and investigating their potential applications in telecom networks. Specifically, the aim of this study is to offer an introduction to the potential utilization of GenAI for critical thinking techniques in mobile networks, while also establishing a foundation for future research.", "citations": 4}
{"title": "A Personalised Learning Tool for Physics Undergraduate Students Built On a Large Language Model for Symbolic Regression", "year": 2024, "authors": "Yufan Zhu, Zi-Yu Khoo, Jonathan Sze Choong Low, St\u00e9phane Bressan", "url": "https://www.semanticscholar.org/paper/c1254758f6d1ee71f5477e44ac86fabab50581a5", "relevance": 3, "abstract": "Interleaved practice enhances the memory and problem-solving ability of students in undergraduate courses. We introduce a personalized learning tool built on a Large Language Model (LLM) that can provide immediate and personalized attention to students as they complete homework containing problems interleaved from undergraduate physics courses. Our tool leverages the dimensional analysis method, enhancing students\u2019 qualitative thinking and problem-solving skills for complex phenomena. Our approach combines LLMs for symbolic regression with dimensional analysis via prompt engineering and offers students a unique perspective to comprehend relationships between physics variables. This fosters a broader and more versatile understanding of physics and mathematical principles and complements a conventional undergraduate physics education that relies on interpreting and applying established equations within specific contexts. We test our personalized learning tool on the equations from Feynman\u2019s lectures on physics. Our tool can correctly identify relationships between physics variables for most equations, underscoring its value as a complementary personalized learning tool for undergraduate physics students.", "citations": 4}
{"title": "On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts", "year": 2023, "authors": "Qineng Wang, Zihao Wang, Ying Su, Yangqiu Song", "url": "https://www.semanticscholar.org/paper/95dae66002ebe9e560c01f6aa1ad412d7362c15c", "relevance": 3, "abstract": "Two ways has been discussed to unlock the reasoning capability of a large language model. The first one is prompt engineering and the second one is to combine the multiple inferences of large language models, or the multi-agent discussion. Theoretically, this paper justifies the multi-agent discussion mechanisms from the symmetry of agents. Empirically, this paper reports the empirical results of the interplay of prompts and discussion mechanisms, revealing the empirical state-of-the-art performance of complex multi-agent mechanisms can be approached by carefully developed prompt engineering. This paper also proposes a scalable discussion mechanism based on conquer and merge, providing a simple multi-agent discussion solution with simple prompts but state-of-the-art performance.", "citations": 2}
{"title": "Optimizing Large Language Models: A Deep Dive into Effective Prompt Engineering Techniques", "year": 2025, "authors": "Minjun Son, Yun-Jae Won, Sungjin Lee", "url": "https://www.semanticscholar.org/paper/7f52fb2e07583e1df41f441849577bfc842ff3c4", "relevance": 3, "abstract": "Recent advancements in Natural Language Processing (NLP) technologies have been driven at an unprecedented pace by the development of Large Language Models (LLMs). However, challenges remain, such as generating responses that are misaligned with the intent of the question or producing incorrect answers. This paper analyzes various Prompt Engineering techniques for large-scale language models and identifies methods that can optimize response performance across different datasets without the need for extensive retraining or fine-tuning. In particular, we examine prominent Prompt Engineering techniques including In-Context Learning (ICL), Chain of Thought (CoT), Retrieval-Augmented Generation (RAG), Step-by-Step Reasoning (SSR), and Tree of Thought (ToT), and we apply these techniques to leading LLMs such as Gemma2, LlaMA3, and Mistral. The performance of these models was evaluated using the AI2 Reasoning Challenge (ARC), HellaSwag, Massive Multitask Language Understanding (MMLU), TruthfulQA, Winogrande, and Grade School Math (GSM8k) datasets across metrics such as BLEU, ROUGE, METEOR, BLEURT, and BERTScore. The experimental results indicate that the most suitable Prompt Engineering technique can vary depending on the characteristics of each dataset. Specifically, for datasets emphasizing mathematical and logical reasoning, Prompt Engineering strategies centered around CoT, SSR, and ToT were found to be advantageous. For datasets focusing on natural language understanding, ICL-centric strategies were more effective, while RAG-based strategies were beneficial for datasets where factual accuracy is crucial. However, it was also observed that the optimal combination of Prompt Engineering techniques could differ depending on the specific LLM, indicating that fine-tuning the Prompt Engineering approach to the model and dataset is essential for achieving the best performance. The findings indicate that as LLMs become more advanced, their reliance on Prompt Engineering (PE) techniques diminishes, yet the magnitude of their performance improvement when PE strategies are applied increases. Furthermore, these advanced models tend to depend less on ICL techniques while exhibiting a greater reliance on RAG strategies. It is also evident that implementing RAG with PE-based preprocessing yields superior performance enhancements compared to the mere application of RAG on raw data.", "citations": 17}
{"title": "Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability and Compliance Checks", "year": 2024, "authors": "Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi", "url": "https://www.semanticscholar.org/paper/8f5222a2471e9f559b84a4c250a1cf86dd10130e", "relevance": 3, "abstract": "Ensuring that Software Requirements Specifications (SRS) align with higher-level organizational or national requirements is vital, particularly in regulated environments such as finance and aerospace. In these domains, maintaining consistency, adhering to regulatory frameworks, minimizing errors, and meeting critical expectations are essential for the reliable functioning of systems. The widespread adoption of large language models (LLMs) highlights their immense potential, yet there remains considerable scope for improvement in retrieving relevant information and enhancing reasoning capabilities. This study demonstrates that integrating a robust Graph-RAG framework with advanced prompt engineering techniques, such as Chain of Thought and Tree of Thought, can significantly enhance performance. Compared to baseline RAG methods and simple prompting strategies, this approach delivers more accurate and context-aware results. While this method demonstrates significant improvements in performance, it comes with challenges. It is both costly and more complex to implement across diverse contexts, requiring careful adaptation to specific scenarios. Additionally, its effectiveness heavily relies on having complete and accurate input data, which may not always be readily available, posing further limitations to its scalability and practicality.", "citations": 10}
{"title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models", "year": 2025, "authors": "Maxwell J. Yin, Dingyi Jiang, Yongbing Chen, Boyu Wang, Charles Ling", "url": "https://www.semanticscholar.org/paper/80a671ac080b11888e2861fc2f31ff01cd71123d", "relevance": 3, "abstract": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging natural language process problem yet highly desirable in many real-life applications. Existing CoT knowledge distillation methods often suffer from overly conservative memorization in smaller LLMs, leading to low generalization confidence. As fully preserving the CoT ability of teacher model is impossible, we hypothesize that adversarial CoT fine-tuning is crucial for developing smaller LLM with robust CoT generalization. To this end, we propose \\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA pioneers two CoT improvements in smaller LLM: (1) Recovering the domain-invariant feature insight which typically lost during distillation with domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT prompt engineering by employing domain-adversarial approaches. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks. Moreover, our empirical findings reveal that the smaller LLM, when leveraging PRADA, aligns closely with domain knowledge, thereby improving the explainability of our approach.", "citations": 6}
{"title": "Performance Analysis of Prompt-Engineering Techniques for Large Language Model", "year": 2025, "authors": "Minjun Son, Sungjin Lee", "url": "https://www.semanticscholar.org/paper/981196b2a6e691673ca6f0249e770c2801070d7f", "relevance": 3, "abstract": "In this paper, we analyze the performance of three large language models (LLMs) - Llama3, Gemma2, and Mistral - using various combinations of prompt engineering techniques, focusing on the TruthfulQA and Winogrande datasets. In TruthfulQA, which emphasizes evaluating the truthfulness and cognitive errors of language models, the Chain of Thought (CoT) and Retrieval-Augmented Generation (RAG) techniques demonstrated superior performance. On the other hand, in Wino-grande, which assesses contextual reasoning abilities based on common-sense knowledge, CoT and In-Context Learning (ICL) were found to be effective. The performance analysis by model revealed that Llama3 exhibited the most significant improvement when prompt engineering techniques were applied. Meanwhile, Gemma2 achieved the highest overall performance across all evaluation metrics. This study highlights the importance of selecting appropriate prompt engineering techniques tailored to each model to maximize LLM performance, demonstrating that effective combinations of techniques can substantially enhance the models' reasoning abilities and accuracy.", "citations": 3}
{"title": "Prompt engineering for accurate statistical reasoning with large language models in medical research", "year": 2025, "authors": "S. Vilakati", "url": "https://www.semanticscholar.org/paper/e02864ff6afa0e509a708b68b8dfa5ec1b825a0d", "relevance": 3, "abstract": "Background The integration of generative artificial intelligence (AI), particularly large language models (LLMs), into medical statistics offers transformative potential. However, it also introduces risks of erroneous responses, especially in tasks requiring statistical rigor. Objective To evaluate the effectiveness of various prompt engineering strategies in guiding LLMs toward accurate and interpretable statistical reasoning in biomedical research. Methods Four prompting strategies: zero-shot, explicit instruction, chain-of-thought, and hybrid were assessed using artificial datasets involving descriptive and inferential statistical tasks. Outputs from GPT-4.1 and Claude 3.7 Sonnet were evaluated using Microsoft Copilot as an LLM-as-a-judge, with human oversight. Results Zero-shot prompting was sufficient for basic descriptive tasks but failed in inferential contexts due to lack of assumption checking. Hybrid prompting, which combines explicit instructions, reasoning scaffolds, and format constraints, consistently produced the most accurate and interpretable results. Evaluation scores across four criteria\u2013assumption checking, test selection, output completeness, and interpretive quality confirmed the superiority of structured prompts. Conclusion Prompt design is a critical determinant of output quality in AI-assisted statistical analysis. Hybrid prompting strategies should be adopted as best practice in medical research to ensure methodological rigor and reproducibility. Additional testing with newer models, including Claude 4 Sonnet, Claude 4 Opus, o3 mini, and o4 mini, confirmed the consistency of results, supporting the generalizability of findings across both Anthropic and OpenAI model families. This study highlights prompt engineering as a core competency in AI-assisted medical research and calls for the development of standardized prompt templates, evaluation rubrics, and further studies across diverse statistical domains to support robust and reproducible scientific inquiry.", "citations": 3}
{"title": "LLM Prompt Engineering for IEEE 802.11 DCF Optimization", "year": 2025, "authors": "Jingyi Zuo, Qiao Lan, Ziyang Guo, Peng Liu, Jian Song", "url": "https://www.semanticscholar.org/paper/2e6a67c3e50304ae2852accfa27843696033f603", "relevance": 3, "abstract": "The emergence of large-language models (LLMs) has catalyzed paradigm shifts in addressing engineering challenges. Although initial attempts have explored LLMs in wireless communications, existing approaches primarily focus on na\u00efve prompts or fine-tuning for specific tasks such as protocol comprehension and resource allocation. In this paper, we introduce the first LLM reasoning framework specifically designed for optimizing the Distributed Coordination Function (DCF) in Wi-Fi networks. Our framework leverages inference-time computing to exploit the reasoning capabilities of LLMs, eliminating the need for task-specific fine-tuning. We instantiate our design in the context of low-delay channel access for next-generation Wireless Local Area Networks (WLANs), implementing reasoning pipelines with varying computational costs, including zero-shot, in-context learning (ICL) and chain-of-thought (CoT) techniques. Experimental results using leading LLMs, specifically Qwen2.5 and DeepSeek-R1, demonstrate the framework\u2019s superiority over traditional protocol benchmarks, achieving an 89.76% reduction in tail delay with CoT prompting. We further analyze the scaling law and the trade-off between computational cost and reasoning performance, providing practical insights for future research in this domain.", "citations": 0}
{"title": "Exploring the Potential of Anomaly Detection Through Reasoning with Large Language Models", "year": 2025, "authors": "Sungjune Park, Daeseon Choi", "url": "https://www.semanticscholar.org/paper/ceafc179abc5b07ae0eb92a9ce8e60c32798ad44", "relevance": 3, "abstract": "In recent years, anomaly detection in digital environments has become a critical research area due to issues such as spam messages and fake news, which can lead to privacy breaches, social disruption, and undermined information reliability. Traditional anomaly detection models often require specific training for each task, resulting in significant time and resource consumption and limited flexibility. This study explores the use of Prompt Engineering with Transformer-based Large Language Models (LLMs) to address these challenges more efficiently. By comparing techniques such as Zero-shot, Few-shot, Chain-of-Thought (CoT), Self-Consistency (SC), and Tree-of-Thought (ToT) prompting, the study identifies CoT and SC as particularly effective, achieving up to 0.96 accuracy in spam detection without the need for task-specific training. However, ToT exhibited limitations due to biases and misinterpretation. The findings emphasize the importance of selecting appropriate prompting strategies to optimize LLM performance across various tasks, highlighting the potential of Prompt Engineering to reduce costs and improve the adaptability of anomaly detection systems. Future research is needed to explore the broader applicability and scalability of these methods. Additionally, this study includes a survey of Prompt Engineering techniques applicable to anomaly detection, examining strategies such as Self-Refine and Retrieval-Augmented Generation to further enhance detection accuracy and adaptability.", "citations": 1}
{"title": "Prompt Engineering in Large Language Models: A Systematic Survey of Optimization Techniques and Real-World Applications", "year": 2025, "authors": "Susmith Barigidad", "url": "https://www.semanticscholar.org/paper/5860698406f0488ff05c52fda0be54f161591db0", "relevance": 3, "abstract": "Consequently, over the past decade, prompt engineering as a transformal technique has emerged to optimally facilitate large language models (LLMs) and vision language models (VLMs) for their transferable usage in various fields without the requirement of model retraining. The focus of this paper is on a thorough analysis of the optimization techniques available in prompt engineering field such as Zero Shot, Few Shot prompting, Chain of Thought, Auto CoT, Logical CoT (LogiCoT) and Retrieval augmented generation (RAG). These techniques boost the performance of LLMs on real-world use cases such as natural language understanding, commonsense reasoning and general solving sophisticated problems across domains in healthcare, finance, education and legal system.Additionally, this paper studies the biases, fairnes, and hallucination problems in LLM outputs and thus highlights how optimized prompt strategies can resolve them and facilitate the model generalization. It also explains the importance of interpretability in AI systems and what current limitations are, as well as the need to use transparent prompt engineering approaches.Some novel research directions suggested in the paper are meta learning for dynamic prompt adaptation, hybrid models that combine few of the techniques for optimized task performance, and the feasibility of an autonomous prompt engineering system. This research push forward to understanding these optimization techniques and in doing so, will help put forward more efficient and equitable LLM deployment across different applications. The conlcusion of the study is that putting the effort into prompt engineering will greatly push AI capabilities, while developing fair and interpretable systems.", "citations": 0}
{"title": "Review of Prompt Engineering Techniques in Finance: An Evaluation of Chain-of-Thought, Tree-of-Thought, and Graph-of-Thought Approaches", "year": 2025, "authors": "Satyadhar Joshi", "url": "https://www.semanticscholar.org/paper/193c2665bcae1fe00998e79206182d44d58ea8d3", "relevance": 3, "abstract": "Recent Advances and Evaluation Techniques in Prompt Engineering for Large Language Models is discussed in this work. This paper surveys recent advances in prompt engineering, including chain-of-thought, tree-of-thought, and graph-of-thought techniques, and reviews over 100 contemporary sources on evaluation metrics, real-world applications, and risks. This paper presents a comprehensive review and evaluation of advanced prompt engineering techniques for financial decision-making using Large Language Models (LLMs). We systematically analyze Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT) prompting methods across six critical financial tasks: risk assessment, portfolio optimization, fraud detection, regulatory compliance, earnings analysis, and derivative pricing. Furthermore, we delve into the crucial aspect of prompt evaluation, discussing key quantitative and qualitative metrics and the tools available for assessing prompt effectiveness, relevance, and safety. We systematically analyze these methods across key financial tasks including risk assessment, portfolio optimization, and fraud detection. Our experimental results demonstrate that structured prompting approaches significantly outperform traditional methods, with Graph-of-Thought achieving 15-25% higher accuracy in complex financial reasoning tasks compared to baseline approaches across literature. Our review of literature also suggest that results demonstrate that structured prompting approaches significantly outperform traditional methods, with Graph-of-Thought achieving 20-25% higher accuracy in complex financial reasoning tasks while reducing hallucination rates by 25-30% as found in the literature. We also comment on FINEVAL, a novel evaluation framework incorporating 12 financial-specific metrics spanning three dimensions: basic quality (accuracy, relevance, fluency), financial validity (regulatory compliance, risk sensitivity), and advanced reasoning (logical soundness, argument depth). The architecture in literature integrates real-time regulatory checks, dynamic prompt optimization, and domain-specific modules for financial applications, achieving 20-25ms latency for CoT paths and 80-90% GPU utilization for ToT operations. Key findings reveal that while 60-65% of surveyed financial institutions are experimenting with CoT, only 10-15% have explored GoT due to computational costs (0.12/query) and skill gaps. We project through studying litreature that by 2030, 60-80% of Tier-1 banks will deploy GoT systems, yielding 30-40% faster M&A due diligence. The paper concludes with strategic recommendations for workforce upskilling (30-50 hour curricula), and risk management protocols, while highlighting emerging challenges in explainability, adversarial robustness, and cross-border compliance. This is a pure review paper and all results are from the cited literature.", "citations": 0}
{"title": "AI-assisted HAZOP - Can prompt engineering deliver a credible HAZOP assessment", "year": 2025, "authors": "Thomas Wilkinson, Luke Hankins, Mathew Wylie, Benjamin Fulford", "url": "https://www.semanticscholar.org/paper/2913c543c49432f80755a9410c20dcf1e3917a75", "relevance": 3, "abstract": "As interest in applying artificial intelligence (AI) to safety engineering grows, practical questions remain about its reliability and rigour. This research explores the use of advanced prompt engineering techniques to automate hazard and operability (HAZOP) analysis to refine the output of a large language model (LLM). Focusing on a deployable portable water production system as a case study, we investigate whether LLMs can support the early stages of hazard identification, augmenting the HAZOP process. Our approach integrates multi-step prompting, chain of thought, atom of thought, and few-shot examples to support reasoning and improve the quality of generated outputs. The results are subjected to system expert review to evaluate coverage, accuracy, and usefulness. This study provides an important step in assessing how LLMs can augment traditional safety workflows. We share key lessons from the experiment, including prompt design strategies, system limitations, and practical insights into deploying LLMs in safety assessment. It offers safety professionals a grounded view of where current AI tools can assist in the safety assessment process and where caution is still required.", "citations": 0}
{"title": "ASSESSING A FINE-TUNED SCRUM AI AGENT: ACCURACY, UTILITY, AND EXPERT VALIDATION", "year": 2025, "authors": "Yama\u00e7 Kaya, Recep Suha Selcuk", "url": "https://www.semanticscholar.org/paper/0714b7a89860532ffad986e69f981393fff04dd7", "relevance": 3, "abstract": "Purpose: This study aims to address the inaccuracies present in default large language models (LLMs) concerning Scrum and Agile methodologies by developing and evaluating a specialized AI model fine-tuned for these domains.\n\u00a0\nTheoretical Framework: Grounded in Agile principles and Scrum frameworks, this research integrates systems thinking and pattern languages to enhance the AI model's understanding and application of Scrum practices.\n\u00a0\nDesign/Methodology/Approach: The research commenced with a pre-trained LLM, which underwent targeted fine-tuning using a curated dataset comprising established Scrum patterns, systems thinking principles, and real-world scenarios from authoritative Scrum literature. Advanced prompt engineering techniques, including Chain of Thought (CoT) and Role-Playing, were employed to enhance the model's reasoning and context-specific response generation. The model's performance was assessed through quantitative metrics and qualitative evaluations by Scrum experts.\n\u00a0\nFindings: The fine-tuned model demonstrated superior performance over general-purpose LLMs in generating accurate, relevant, and actionable responses aligned with empirical Scrum practices. Quantitative analyses revealed significant improvements in accuracy and context relevance, while qualitative feedback from experts highlighted the model's enhanced understanding of nuanced Scrum scenarios.\n\u00a0\nResearch, Practical & Social Implications: This study underscores the potential of domain-specific fine-tuning in mitigating misinformation propagated by general-purpose LLMs. Practically, the developed model serves as a reliable tool for Scrum practitioners seeking guidance, thereby improving Agile implementation outcomes. Socially, the research contributes to the broader discourse on responsible AI deployment, emphasizing the need for contextual awareness in AI applications.\n\u00a0\nOriginality/Value: This research pioneers the fine-tuning of LLMs specifically for Scrum and Agile methodologies, offering a novel solution to the inaccuracies prevalent in default models. The integration of advanced prompt engineering techniques and the emphasis on empirical Scrum practices contribute uniquely to both AI and Agile communities.", "citations": 2}
{"title": "EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit", "year": 2025, "authors": "Chang Liu, D. Chitnis", "url": "https://www.semanticscholar.org/paper/b1d51beb7219ceb38df63b46ddc867787c654239", "relevance": 3, "abstract": "The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.", "citations": 4}
{"title": "DeceptiLens: an Approach supporting Transparency in Deceptive Pattern Detection based on a Multimodal Large Language Model", "year": 2025, "authors": "Emre Kocyigit, Arianna Rossi, A. Sergeeva, Claudia Negri Ribalta, Ali Farjami, Gabriele Lenzini", "url": "https://www.semanticscholar.org/paper/d0ee4d9778e22aa6efdcbb38a94545b685c44552", "relevance": 3, "abstract": "To detect deceptive design patterns on UIs, traditional artificial intelligence models, such as machine learning, have limited coverage and a lack of multimodality. In contrast, the capabilities of Multimodal Large Language Model (MM-LLM) can achieve wider coverage with superior performance in the detection, while providing reasoning behind each decision. We propose and implement an MM-LLM-based approach (DeceptiLens) that analyzes UIs and assesses the presence of deceptive design patterns. We utilize Retrieval Augmented Generation (RAG) process in our design and task the model with capturing the deceptive patterns, classifying its category, e.g., false hierarchy, confirmshaming, etc., and explaining the reasoning behind the classifications by employing recent prompt engineering techniques, such as Chain-of-Thought (CoT). We first create a dataset by collecting UI screenshots from the literature and web sources and quantify the agreement between the model\u2019s outputs and a few experts\u2019 opinions. We additionally ask experts to gauge the transparency of the system\u2019s explanations for its classifications in terms of recognized metrics of clarity, correctness, completeness, and verifiability. The results indicate that our approach is capable of capturing the deceptive patterns in UIs with high accuracy while providing clear, correct, complete, and verifiable justifications for its decisions. We additionally release two curated datasets, one with expert-labeled UIs with deceptive design patterns, and one with AI-based generated explanations. Lastly, we propose recommendations for future improvement of the approach in various contexts of use.", "citations": 1}
{"title": "Adapting LLM to Generate Vulnerability Descriptions and Repair Suggestions", "year": 2025, "authors": "Wenxin Tao, Yekun Ke, Zhaohui Dong, Yanlong Li, Xiaohong Su", "url": "https://www.semanticscholar.org/paper/d22f245386384979310b05579788f3efa50807ab", "relevance": 3, "abstract": "Software vulnerabilities pose serious threats to the security of modern software systems. Existing detection tools can flag or pinpoint vulnerability but offer little guidance on how to fix them. In this paper, we leverage the generative and reasoning strengths of LLMs to bridge this gap, combining prompt tuning, retrieval-augmented generation, and chain-of-thought techniques to produce both clear vulnerability descriptions and concrete repair suggestions. We evaluated five mainstream LLMs on the BigVul dataset, and the combination of RAG and CoT methods achieves leading performance across multiple metrics in both the vulnerability description generation and repair suggestion generation tasks.", "citations": 0}
{"title": "Prompt Engineering Techniques for Language Model Reasoning Lack Replicability", "year": 2025, "authors": "Laur\u00e8ne Vaugrante, Mathias Niepert, Thilo Hagendorff", "url": "https://www.semanticscholar.org/paper/b10776b037991292a2c9e23d5a26205acb00bf50", "relevance": 3, "abstract": "", "citations": 0}
{"title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "year": 2025, "authors": "Fares Wael, Youssef Maklad, Ali Hamdi, W. Elsersy", "url": "https://www.semanticscholar.org/paper/0fa5c1ad4dc245cfa36043b19b28a1acf3f7c1a6", "relevance": 3, "abstract": "Finite-State Machines (FSMs) are critical for modeling the operational logic of network protocols, enabling verification, analysis, and vulnerability discovery. However, existing FSM extraction techniques face limitations such as scalability, incomplete coverage, and ambiguity in natural language specifications. In this paper, we propose FlowFSM, a novel agentic framework that leverages Large Language Models (LLMs) combined with prompt chaining and chain-of-thought reasoning to extract accurate FSMs from raw RFC documents. FlowFSM systematically processes protocol specifications, identifies state transitions, and constructs structured rule-books by chaining agent outputs. Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM achieves high extraction precision while minimizing hallucinated transitions, showing promising results. Our findings highlight the potential of agent-based LLM systems in the advancement of protocol analysis and FSM inference for cybersecurity and reverse engineering applications.", "citations": 0}
{"title": "OMPGPT: A Generative Pre-trained Transformer Model for OpenMP", "year": 2024, "authors": "Le Chen, Arijit Bhattacharjee, Nesreen K. Ahmed, Niranjan Hasabnis, Gal Oren, Vy A. Vo, Ali Jannesari", "url": "https://www.semanticscholar.org/paper/7f8d167e00246f8112cf825ea4ca088594d2ce1d", "relevance": 2, "abstract": "Large language models (LLMs)such as ChatGPT have significantly advanced the field of Natural Language Processing (NLP). This trend led to the development of code-based large language models such as StarCoder, WizardCoder, and CodeLlama, which are trained extensively on vast repositories of code and programming languages. While the generic abilities of these code LLMs are useful for many programmers in tasks like code generation, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific model a smarter choice. This paper presents OMPGPT, a novel domain-specific model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we leverage prompt engineering techniques from the NLP domain to create Chain-of-OMP, an innovative strategy designed to enhance OMPGPT's effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms existing large language models specialized in OpenMP tasks and maintains a notably smaller size, aligning it more closely with the typical hardware constraints of HPC environments. We consider our contribution as a pivotal bridge, connecting the advantage of language models with the specific demands of HPC tasks.", "citations": 26}
{"title": "Can LLMs See What I See? A Study on Five Prompt Engineering Techniques for Evaluating UX on a Shopping Site", "year": 2025, "authors": "Subin Shin, Jeesun Oh, Sangwon Lee", "url": "https://www.semanticscholar.org/paper/47d877817d5afbda075ee8b5d4357a66a6b1f6ae", "relevance": 2, "abstract": "Usability testing is essential for improving digital user experiences but has practical limitations in terms of cost-effectiveness. Recent advancements in multimodal Large Language Models (LLMs), like ChatGPT-4, offer new possibilities for UX evaluations. This study investigated the most effective prompt engineering techniques for identifying UX issues in digital interfaces. To achieve this, five prompt engineering techniques were carefully selected based on previous research, and the outputs generated using these techniques were analyzed based on severity assessment criteria. We discovered that Role Prompting and (Zero-Shot) Chain of Thought Prompting were highly effective. Further investigation revealed that a hybrid approach combining both techniques produced the best results. Our findings shed light on the possibility of using multimodal LLM as a UX evaluator, offering meaningful value for future advancements in LLM-based UX evaluations.", "citations": 1}
{"title": "Analyzing the Sensitivity of Prompt Engineering Techniques in Natural Language Interfaces for 2.5D Software Visualization", "year": 2025, "authors": "Daniel Atzberger, Adrian Jobst, M. Tytarenko, Willy Scheibel, J\u00fcrgen D\u00f6llner, Tobias Schreck", "url": "https://www.semanticscholar.org/paper/a911bbbeda7366420b7642b7361523f9a8e12d01", "relevance": 2, "abstract": "Natural Language Interfaces (NLIs) backed by Large Language Models (LLMs) are used to interact with visualizations through natural language queries. Using the specific example of 2.5D treemaps, the Delphi tool was recently presented, introducing an interactive 2.5D visualization with an accompanying chat interface, where the LLM can react to user input and adapt the visualization at its own discretion. While Delphi has demonstrated effectiveness, the authors have not included an evaluation of the LLM's performance with respect to its prompt and specific task types. In this study, we systematically evaluate the impact of prompt engineering on Delphi's ability to answer factual questions related to data and visualization. Specifically, we investigate the effect of the Chain-of-Thought prompting technique by employing a questionnaire comprising 40 questions across ten low-level analytic tasks. Our findings aim to refine prompt design methodologies and enhance the usability and effectiveness of NLIs in advanced visualization systems.", "citations": 2}
{"title": "Comparing the Effectiveness of Prompt Engineering Techniques for Automated Essay Scoring in Social Studies", "year": 2025, "authors": "Yunha Jung, Sun-Guen Baek", "url": "https://www.semanticscholar.org/paper/082a40bef03c8ac99cf656a322b81162fd123227", "relevance": 2, "abstract": "This study aimed to explore the effectiveness of generative AI-based automated scoring in essay-type assessments in social studies by applying various prompt engineering techniques and evaluating their performance and usefulness. Based on Bloom\u2019s taxonomy (remember & understand / apply & analyze / evaluate & create), three essay-items and rubrics were developed, and ten types of prompts were designed. Automated scoring results based on GPT model were quantitatively and qualitatively compared to teacher scoring results. The key findings are as follows. First, the effectiveness of prompt types varied depending on the cognitive level required by the item. In particular, for lower-order items, prompts such as \u201cChain of Thought\u201d and \u201cMulti-Zero-Shot\u201d that encouraged step-by-step reasoning showed higher accuracy. Second, scoring performance improved when rubrics and Zero-Shot prompts were written clearly and specifically. Third, prompts with added sample answers or few-shot prompts performed worse than Zero-Shot prompts, despite the additional input cost. Finally, when tested on new data, the optimal prompts demonstrated \u2019almost perfect\u2018 agreement with teacher scoring (QWK=0.88, 0.86 for Items 1 and 2) and \u2019substantial\u2019 agreement (QWK=0.63 for Item 3). In addition, qualitative analyses of disagreement cases were conducted, and the usefulness of each prompt technique was discussed in detail.", "citations": 0}
{"title": "Emergent Abilities of Large Language Models", "year": 2022, "authors": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, O. Vinyals, P. Liang, J. Dean, W. Fedus", "url": "https://www.semanticscholar.org/paper/dac3a172b504f4e33c029655e9befb3386e5f63a", "relevance": 2, "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "citations": 3210}
{"title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models", "year": 2023, "authors": "Fobo Shi, Peijun Qing, D. Yang, Nan Wang, Youbo Lei, H. Lu, Xiaodong Lin", "url": "https://www.semanticscholar.org/paper/2d338cdd12091814dec11155d3f6f848d7bab4d8", "relevance": 2, "abstract": "Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt\"Let's think step by step\", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at \\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}", "citations": 16}
{"title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models", "year": 2024, "authors": "Benji Peng, Ziqian Bi, Qian Niu, Ming Liu, Pohsun Feng, Tianyang Wang, Lawrence K.Q. Yan, Yizhu Wen, Yichao Zhang, Caitlyn Heqi Yin", "url": "https://www.semanticscholar.org/paper/1c9cfc4c659b5daae67d60a55030cf48cd572ce0", "relevance": 1, "abstract": "Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.", "citations": 28}
{"title": "A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities", "year": 2025, "authors": "Lu Xiang, Yang Zhao, Yaping Zhang, Chengqing Zong", "url": "https://www.semanticscholar.org/paper/cc66970b020e40867ad259d9b450433366e77048", "relevance": 1, "abstract": "Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration. However, a systematic understanding of their integration into diverse disciplines remains underexplored. This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability. From a technical standpoint, key methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration are examined, which enhance the adaptability and effectiveness of LLMs in discipline-specific contexts. From the perspective of their applicability, this paper explores how LLMs are contributing to various disciplines including mathematics, physics, chemistry, biology, and the humanities and social sciences, demonstrating their role in discipline-specific tasks. The prevailing challenges are critically examined and the promising research directions are highlighted alongside the recent advances in LLMs. By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies.", "citations": 4}
{"title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions", "year": 2025, "authors": "Armaugan Amcalar, Eyup Cinar", "url": "https://www.semanticscholar.org/paper/be51cf555ec5237e5853adf3ab9b3f80b148d594", "relevance": 1, "abstract": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.", "citations": 0}
{"title": "SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world Human-Machine Interactions", "year": 2024, "authors": "Yaqi Wang, Haipei Xu", "url": "https://www.semanticscholar.org/paper/371a1dee2c9942457d0fd864dd01b6593e83027d", "relevance": 1, "abstract": "Recently, as Large Language Models (LLMs) have shown impressive emerging capabilities and gained widespread popularity, research on LLM-based search agents has proliferated. In real-world situations, users often input contextual and highly personalized queries to chatbots, challenging LLMs to capture context and generate appropriate answers. However, much of the prior research has not focused specifically on authentic human-machine dialogue scenarios. It also ignores the important balance between response quality and computational cost by forcing all queries to follow the same agent process. To address these gaps, we propose a Strategy-Router Search Agent (SRSA), routing different queries to appropriate search strategies and enabling fine-grained serial searches to obtain high-quality results at a relatively low cost. To evaluate our work, we introduce a new dataset, Contextual Query Enhancement Dataset (CQED), comprising contextual queries to simulate authentic and daily interactions between humans and chatbots. Using LLM-based automatic evaluation metrics, we assessed SRSA\u2019s performance in terms of informativeness, completeness, novelty, and actionability. To conclude, SRSA provides an approach that resolves the issue of simple serial searches leading to degenerate answers for lengthy and contextual queries, effectively and efficiently parses complex user queries, and generates more comprehensive and informative responses without fine-tuning an LLM. The code is available at https://anonymous.4open.science/r/SRSA-3A04/.", "citations": 1}
{"title": "Judgment-of-Thought Prompting: A Courtroom-Inspired Framework for Binary Logical Reasoning with Large Language Models", "year": 2024, "authors": "Sungjune Park, Heehwan Kim, Haehyun Cho, Daeseon Choi", "url": "https://www.semanticscholar.org/paper/0bfa7c89253235d2822a3fd45dfcc9f2e7787984", "relevance": 1, "abstract": "This paper proposes a novel prompting approach, Judgment of Thought (JoT), specifically tailored for binary logical reasoning tasks. Despite advances in prompt engineering, existing approaches still face limitations in handling complex logical reasoning tasks. To address these issues, JoT introduces a multi-agent approach with three specialized roles$\\unicode{x2010}$$\\unicode{x2010}$$\\unicode{x2010}$lawyer, prosecutor, and judge$\\unicode{x2010}$$\\unicode{x2010}$$\\unicode{x2010}$where a high-level model acts as the judge, and lower-level models serve as lawyer and prosecutor to systematically debate and evaluate arguments. Experimental evaluations on benchmarks such as BigBenchHard and Winogrande demonstrate JoT's superior performance compared to existing prompting approaches, achieving notable improvements, including 98\\% accuracy in Boolean expressions. Also, our ablation studies validate the critical contribution of each role, iterative refinement loops, and feedback mechanisms. Consequently, JoT significantly enhances accuracy, reliability, and consistency in binary reasoning tasks and shows potential for practical applications.", "citations": 0}
{"title": "Reasoning for Translation: Comparative Analysis of Chain-of-Thought and Tree-of-Thought Prompting for LLM Translation", "year": 2025, "authors": "Lam Nguyen, Yang Xu", "url": "https://www.semanticscholar.org/paper/5da888df97c66fcf4f5a5bbf48b56510f09d2f00", "relevance": 1, "abstract": "As Large Language Models (LLMs) continue to advance in capability, prompt engineering has emerged as a crucial method for optimizing their performance on specialized tasks. While prompting strategies like Zero-shot, Few-shot, Chain-of-Thought, and Tree-of-Thought have demonstrated significant improvements in reasoning tasks, their application to machine translation has received relatively less attention. This paper systematically evaluates these prompting techniques across diverse language pairs and domains, measuring their effect on translation quality. Our findings reveal substantial performance variations between prompting methods, with certain strategies offering consistent improvements for specific language directions and complexity levels. These results provide valuable insights for developing more effective LLM-based translation systems without requiring model fine-tuning and complement existing works in the field.", "citations": 3}
{"title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022, "authors": "Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, Ed H. Chi, Denny Zhou", "url": "https://api.semanticscholar.org/CorpusId:247595263", "relevance": 1, "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).", "citations": 5879}
{"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "year": 2022, "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao", "url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d", "relevance": 1, "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io", "citations": 5740}
{"title": "PAL: Program-aided Language Models", "year": 2022, "authors": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig", "url": "https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7", "relevance": 1, "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .", "citations": 631}
{"title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective", "year": 2023, "authors": "Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, Liwei Wang", "url": "https://www.semanticscholar.org/paper/c2260403fd5cb2de73491323433e48b6ec36872c", "relevance": 1, "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.", "citations": 368}
{"title": "Large Language Model Guided Tree-of-Thought", "year": 2023, "authors": "Jieyi Long", "url": "https://www.semanticscholar.org/paper/bda605928d6ebe4db906e69ab5d343df75918727", "relevance": 1, "abstract": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.", "citations": 291}
{"title": "Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Representation Learning", "year": 2025, "authors": "Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu", "url": "https://www.semanticscholar.org/paper/8f00852c0b797e63e734cc873b41e1ae4f663aeb", "relevance": 1, "abstract": "Large Language Models (LLMs) have achieved remarkable success across various domains. However, they still face significant challenges, including high computational costs for training and limitations in solving complex reasoning problems. Although existing methods have extended the reasoning capabilities of LLMs through structured paradigms, these approaches often rely on task-specific prompts and predefined reasoning processes, which constrain their flexibility and generalizability. To address these limitations, we propose a novel framework that leverages graph learning to enable more flexible and adaptive reasoning capabilities for LLMs. Specifically, this approach models the reasoning process of a problem as a graph and employs LLM-based graph learning to guide the adaptive generation of each reasoning step. To further enhance the adaptability of the model, we introduce a Graph Neural Network (GNN) module to perform representation learning on the generated reasoning process, enabling real-time adjustments to both the model and the prompt. Experimental results demonstrate that this method significantly improves reasoning performance across multiple tasks without requiring additional training or task-specific prompt design. Code can be found in https://github.com/zch65458525/L2T.", "citations": 0}
{"title": "UL2: Unifying Language Learning Paradigms", "year": 2022, "authors": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garc\u00eda, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, H. Zheng, Denny Zhou, N. Houlsby, Donald Metzler", "url": "https://www.semanticscholar.org/paper/b21670e8061a06ab97e7d6052c9345a326e84ff8", "relevance": 1, "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.", "citations": 363}
{"title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "year": 2021, "authors": "Laria Reynolds, Kyle McDonell", "url": "https://www.semanticscholar.org/paper/ac3cdb50606f7770eef8e4cd951840a4f71287a0", "relevance": 1, "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models\u2019 novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.", "citations": 1193}
{"title": "Fast Thinking with Structured Prompts: Enabling LLM Reasoning Without Chain-of-Thought Generation", "year": 2025, "authors": "Kirill Morozov, Liubov Chubarova, Irina Piontkovskaya", "url": "https://www.semanticscholar.org/paper/666da04f57e2863f93834dd96f144e8323da5f33", "relevance": 1, "abstract": "The emergence of complex reasoning abilities in large language models (LLMs) has sparked great interest, and a variety of prompting techniques was proposed to coax them into emulating human thought processes. In this work, we introduce Think Node-by-Node , a graph-based reasoning framework inspired by mind maps, flowcharts, and other visual aids that help humans tackle complex problems. Rather than generating images directly, our approach leverages standard graph-building and rendering libraries, and requires no fine-tuning, only the model\u2019s native coding capabilities. We further explore a \u201cFast Thinking\u201d regime, in which a graph-reasoning example provided in the prompt, but the model generates the answers directly, without the full thought process reconstruction. Surprisingly, this approach leads to significant improvement upon baseline in general-knowledge tasks. Remarkably, Think Node-by-Node maintains strong performance even under a strict 25-token budget for answer generation. Across two instruction-tuned LLMs (0.5B and 7B parameters), our Fast-TNbN strategy outperforms baseline prompting techniques, improving accuracy by up to 10%, and exceeds the capabilities of other structured prompting methods under equivalent generation constraints.", "citations": 0}
{"title": "Chain-of-Thought Reasoning Without Prompting", "year": 2024, "authors": "Xuezhi Wang, Denny Zhou", "url": "https://www.semanticscholar.org/paper/c8b1206ef8e6fdebd3b9ad2165937256ab8b5652", "relevance": 1, "abstract": "In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding.", "citations": 228}
{"title": "A Model Is Not Built By A Single Prompt: LLM-Based Domain Modeling With Question Decomposition", "year": 2024, "authors": "Ru Chen, Jingwei Shen, Xiao He", "url": "https://www.semanticscholar.org/paper/8ff156b2ecd8cc2aa56b9303d3cd0ccf95920d41", "relevance": 1, "abstract": "Domain modeling, a crucial part of model-driven engineering, demands extensive domain knowledge and experience from engineers. When the system description is highly complicated, the modeling task can become particularly challenging and time-consuming. Large language Models(LLMs) can assist by automatically generating an initial object model from the system description. Although LLMs have demonstrated remarkable code-generation ability, they still struggle with model-generation using a single prompt. In real-world domain modeling, engineers usually decompose complex tasks into easily solvable sub-tasks, significantly controlling complexity and enhancing model quality. Inspired by this, we propose an LLM-based domain modeling approach via question decomposition, similar to developer's modeling process. Following conventional modeling guidelines, we divide the model generation task into several sub-tasks, i.e., class generation, association and aggregation generation, and inheritance generation. For each sub-task, we carefully design the prompt by choosing more efficient query words and providing essential modeling knowledge to unlock the modeling potential of LLMs. To sum up all the sub-tasks solutions, we implemente a proof-of-object tool integrated into the standard Ecore editor that asks LLMs to generate an object model from the system description. We evaluate our approach with 20 systems from different application domains. The preliminary results show that our approach outperforms the single-prompt-based prompt by improving recall values and F1 scores in most systems for modeling the classes, attributes, and relationships.", "citations": 10}
{"title": "Expanding Horizons in Prompt Engineering: Techniques, Frameworks, and Challenges", "year": 2025, "authors": "Nan Wu", "url": "https://www.semanticscholar.org/paper/bc5cac87205c9854c57a694987786a862517c445", "relevance": 1, "abstract": "Abstract\u2014Prompt engineering is a critical method\nfor guiding large language models (LLMs) to perform\ndiverse tasks effectively. This paper reviews prompt\nengineering techniques, including example-driven\napproaches, logic steps, and modular frameworks,\nhighlighting their adaptability and limitations. It\nemphasizes the need for robust evaluation metrics,\naddressing gaps in reasoning quality and task\nadherence. By proposing novel evaluation approaches\nsuch as token-level entropy, this study advances\nunderstanding of prompt effectiveness, paving the\nway for more reliable, ethical, and domain-specific\nLLM applications.\nKeywords\u2014prompt engineering, language models,\nchain of thought, AI benchmark", "citations": 1}
{"title": "CO-STEP: A Prompt Engineering Framework Improving LLM's Response", "year": 2025, "authors": "Tuan Pham, Cuong Doan Cao", "url": "https://www.semanticscholar.org/paper/e1bd4e25c16e29d53b5086eb02f288eb850d8e45", "relevance": 1, "abstract": "", "citations": 0}
{"title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding", "year": 2025, "authors": "Hyunbin Jin, J. Yeom, Seunghyun Bae, Taesup Kim", "url": "https://www.semanticscholar.org/paper/857f3522ac09ba41428aa90e9b97e3d635d615b0", "relevance": 1, "abstract": "Large language models (LLMs) exhibit strong reasoning abilities, often attributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While effective, these methods require labor-intensive prompt engineering, raising the question of whether reasoning can be induced without reliance on explicit prompts. In this work, we unlock the reasoning capabilities of LLMs without explicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a novel decoding strategy that systematically nudges LLMs to continue reasoning, thereby preventing immature reasoning processes. Specifically, we monitor the model's generation and inject a designated phrase whenever it is likely to conclude its response prematurely, before completing the reasoning process. Our experimental evaluations on diverse reasoning benchmarks demonstrate that our proposed strategy substantially improves LLM reasoning capabilities, highlighting the potential of decoding-based interventions as an alternative to traditional prompting techniques.", "citations": 6}
{"title": "Self-Refine: Iterative Refinement with Self-Feedback", "year": 2023, "authors": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark", "url": "https://www.semanticscholar.org/paper/3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "relevance": 1, "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "citations": 2768}
{"title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "year": 2024, "authors": "Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou", "url": "https://www.semanticscholar.org/paper/2b3ad2fdd9d2013119232ee49e6d21eb08474b74", "relevance": 1, "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.", "citations": 292}
{"title": "Large Language Models (LLMs) for Wireless Networks: An Overview from the Prompt Engineering Perspective", "year": 2024, "authors": "Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xi Chen, Hina Tabassum, Xue Liu", "url": "https://www.semanticscholar.org/paper/471e270a6e1943c9bf1472dfdd4c9df7671344e6", "relevance": 1, "abstract": "", "citations": 7}
{"title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models", "year": 2024, "authors": "Maurice Diesendruck, Jianzhe Lin, Shima Imani, Gayathri Mahalingam, Mingyang Xu, Jie Zhao", "url": "https://www.semanticscholar.org/paper/e55110aeaa4f4e86e231fb936d3bb8092d4f018d", "relevance": 1, "abstract": "When LLMs perform zero-shot inference, they typically use a prompt with a task specification, and generate a completion. However, there is no work to explore the possibility of the reverse - going from completion to task specification. In this paper, we employ both directions to perform cycle-supervised learning entirely in-context. Our goal is to create a forward map f : X ->Y (e.g. image ->generated caption), coupled with a backward map g : Y ->X (e.g. caption ->generated image) to construct a cycle-consistency\"loss\"(formulated as an update to the prompt) to enforce g(f(X)) ~= X. The technique, called CyclePrompt, uses cycle-consistency as a free supervisory signal to iteratively craft the prompt. Importantly, CyclePrompt reinforces model performance without expensive fine-tuning, without training data, and without the complexity of external environments (e.g. compilers, APIs). We demonstrate CyclePrompt in two domains: code generation and image captioning. Our results on the HumanEval coding benchmark put us in first place on the leaderboard among models that do not rely on extra training data or usage of external environments, and third overall. Compared to the GPT4 baseline, we improve accuracy from 80.5% to 87.2%. In the vision-language space, we generate detailed image captions which outperform baseline zero-shot GPT4V captions, when tested against natural (VQAv2) and diagrammatic (FigureQA) visual question-answering benchmarks. To the best of our knowledge, this is the first use of self-supervised learning for prompting.", "citations": 3}
{"title": "Language Models are Few-Shot Learners", "year": 2020, "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Ma-teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, I. Sutskever, Dario Amodei", "url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "relevance": 1, "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "citations": 53740}
{"title": "Language Models are Unsupervised Multitask Learners", "year": 2019, "authors": "Alec Radford, Jeff Wu, R. Child, D. Luan, Dario Amodei, I. Sutskever", "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe", "relevance": 1, "abstract": "", "citations": 27120}
{"title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration", "year": 2023, "authors": "Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng Kong", "url": "https://www.semanticscholar.org/paper/bfeda6c7aa7899a80adb01894555b09d24756a59", "relevance": 1, "abstract": "Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.", "citations": 72}
{"title": "Large Language Models Imitate Logical Reasoning, but at what Cost?", "year": 2025, "authors": "Lachlan McGinness, Peter Baumgartner", "url": "https://www.semanticscholar.org/paper/130f88c0498747b38ad593ec7e6310bbeb062275", "relevance": 1, "abstract": "We present a longitudinal study which evaluates the reasoning capability of frontier Large Language Models over an eighteen month period. We measured the accuracy of three leading models from December 2023, September 2024 and June 2025 on true or false questions from the PrOntoQA dataset and their faithfulness to reasoning strategies provided through in-context learning. The improvement in performance from 2023 to 2024 can be attributed to hidden Chain of Thought prompting. The introduction of thinking models allowed for significant improvement in model performance between 2024 and 2025. We then present a neuro-symbolic architecture which uses LLMs of less than 15 billion parameters to translate the problems into a standardised form. We then parse the standardised forms of the problems into a program to be solved by Z3, an SMT solver, to determine the satisfiability of the query. We report the number of prompt and completion tokens as well as the computational cost in FLOPs for open source models. The neuro-symbolic approach significantly reduces the computational cost while maintaining near perfect performance. The common approximation that the number of inference FLOPs is double the product of the active parameters and total tokens was accurate within 10\\% for all experiments.", "citations": 2}
{"title": "Automated Tailoring of Large Language Models for Industry-Specific Downstream Tasks", "year": 2024, "authors": "Shreya Saxena, Siva Prasad, Muneeswaran I, Advaith Shankar, Varun V, Saisubramaniam Gopalakrishnan, Vishal Vaddina", "url": "https://www.semanticscholar.org/paper/d9f222ef7752030163aa0380fefca71d5b27eca0", "relevance": 1, "abstract": "Foundational Large Language Models (LLMs) are pre-trained generally on huge corpora encompassing broad subjects to become versatile and generalize to future downstream tasks. However, their effectiveness falls short when dealing with tasks that are highly specialized to a specific use case. Even when adopting current prompt engineering techniques like few-shot or Chain-of-Thought reasoning prompts, the required level of results is not yet achievable directly with foundational models alone. The alternative approach is to fine-tune the LLM, but a common challenge is the limited availability of task-specific training data. In this talk, we will introduce an end-to-end automated framework to tailor a model to specific downstream tasks for an industry where the first step is to generate task-specific custom data from unstructured documents. Next, we will discuss our optimized distributed training pipeline for fine-tuning LLMs on the generated data. Finally, we will provide an overview of the statistical metrics and customized metrics we employ for assessing the performance of the fine-tuned LLM. This automated framework alleviates the burden of manual adjustments and streamlines the process to provide a model that is fully customized to suit the unique requirements of any specific business use case.", "citations": 5}
